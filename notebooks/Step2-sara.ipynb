{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6ea53c-00ac-4ad8-98ad-3a7fd6f076e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54606fb2-27e4-44dc-9ac8-b19d8a62df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset\n",
    "file_path = r\"C:\\Users\\sosso\\Machine Learning\\Projet\\Phishing_Email.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Nettoyer les colonnes\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Supprimer la colonne inutile\n",
    "df = df.drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
    "\n",
    "# Supprimer les lignes où Email Text est NaN\n",
    "df = df.dropna(subset=[\"Email Text\"])\n",
    "\n",
    "# Encoder la colonne cible Email Type en 0/1\n",
    "df[\"Email Type\"] = df[\"Email Type\"].map({\n",
    "    \"Safe Email\": 0,\n",
    "    \"Phishing Email\": 1\n",
    "})\n",
    "\n",
    "# Vérifier la distribution des classes\n",
    "print(\"Distribution des classes :\")\n",
    "print(df[\"Email Type\"].value_counts())\n",
    "\n",
    "# Séparer features et cible\n",
    "X_text = df[\"Email Text\"]\n",
    "y = df[\"Email Type\"]\n",
    "\n",
    "# Vectorisation du texte (TF-IDF)\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X = tfidf.fit_transform(X_text)\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fec63b9-c7a7-4b24-8a05-203ef6b6dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les modèles\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=2000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "}\n",
    "# Entraîner et évaluer chaque modèle\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"--- {name} ---\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0d6400-e8cc-44c8-be40-3ec453723b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Paramètres Grid Search\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'solver': ['lbfgs']\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': [3, 5, 10, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100],\n",
    "        'max_depth': [10, 20, None],\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'probability': [True]\n",
    "    },\n",
    "}\n",
    "\n",
    "best_models = []\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n Grid Search {name}...\")\n",
    "    \n",
    "    # GridSearch sur les matrices TF-IDF\n",
    "    grid = GridSearchCV(model, param_grids[name], cv=3, scoring='accuracy', n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Best Params': grid.best_params_,\n",
    "        'Accuracy': acc,\n",
    "        'F1': f1\n",
    "    })\n",
    "    \n",
    "    print(f\"    Accuracy: {acc:.4f} | F1-score: {f1:.4f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nRésumé Grid Search :\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c0f9e5-468e-4873-8979-138958ffab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier, StackingClassifier\n",
    "\n",
    "# Top 3 modèles selon l’accuracy\n",
    "top_3 = results_df.nlargest(3, 'Accuracy')['Model'].tolist()\n",
    "estimators = [(name, best_models[name]) for name in top_3]\n",
    "\n",
    "# Voting Classifier (soft voting)\n",
    "voting = VotingClassifier(estimators=estimators, voting='soft')\n",
    "voting.fit(X_train, y_train)\n",
    "y_pred_voting = voting.predict(X_test)\n",
    "acc_voting = accuracy_score(y_test, y_pred_voting)\n",
    "print(f\" Voting: {acc_voting:.4f}\")\n",
    "\n",
    "# Bagging sur le meilleur modèle\n",
    "best_name = results_df.loc[results_df['Accuracy'].idxmax(), 'Model']\n",
    "bagging = BaggingClassifier(best_models[best_name], n_estimators=20, random_state=42)\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging.predict(X_test)\n",
    "acc_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "print(f\" Bagging: {acc_bagging:.4f}\")\n",
    "\n",
    "# Stacking Classifier\n",
    "stacking = StackingClassifier(\n",
    "    estimators=estimators, \n",
    "    final_estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
    "    cv=3\n",
    ")\n",
    "stacking.fit(X_train, y_train)\n",
    "y_pred_stacking = stacking.predict(X_test)\n",
    "acc_stacking = accuracy_score(y_test, y_pred_stacking)\n",
    "print(f\" Stacking: {acc_stacking:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818b0447-3b18-456d-a208-794e76af06f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Créer un DataFrame pour les méthodes d'ensemble\n",
    "ensemble_df = pd.DataFrame([\n",
    "    {'Model': 'Voting', 'Accuracy': acc_voting, 'F1': f1_score(y_test, y_pred_voting)},\n",
    "    {'Model': 'Bagging', 'Accuracy': acc_bagging, 'F1': f1_score(y_test, y_pred_bagging)},\n",
    "    {'Model': 'Stacking', 'Accuracy': acc_stacking, 'F1': f1_score(y_test, y_pred_stacking)},\n",
    "])\n",
    "\n",
    "# Combiner avec les résultats de GridSearch\n",
    "final = pd.concat([results_df[['Model', 'Accuracy', 'F1']], ensemble_df], ignore_index=True)\n",
    "\n",
    "# Afficher le tableau complet\n",
    "print(final.to_string(index=False))\n",
    "\n",
    "# Sauvegarder le tableau en CSV\n",
    "final.to_csv('results.csv', index=False)\n",
    "print(\"\\n Tableau sauvegardé: results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a82d30-972e-453c-bd31-a984d218eee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Graph 1: Comparaison des accuracies\n",
    "plt.figure(figsize=(12, 5))\n",
    "colors = ['skyblue']*len(models) + ['orange', 'green', 'red']  # Couleurs pour base + ensemble\n",
    "plt.bar(final['Model'], final['Accuracy'], color=colors, alpha=0.7)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Comparaison des Modèles')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparison.png', dpi=300)\n",
    "print(\" comparison.png\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Graph 2: Confusion matrices (top 4)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "predictions = {\n",
    "    best_name: best_models[best_name].predict(X_test),  # TF-IDF pour tout\n",
    "    'Voting': y_pred_voting,\n",
    "    'Bagging': y_pred_bagging,\n",
    "    'Stacking': y_pred_stacking\n",
    "}\n",
    "\n",
    "for idx, (name, pred) in enumerate(predictions.items()):\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx], cbar=False)\n",
    "    axes[idx].set_title(name)\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "    axes[idx].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrices.png', dpi=300)\n",
    "print(\" confusion_matrices.png\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df92d598-7724-4a65-a7a9-44e5eb22832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion\n",
    "best_overall = final.loc[final['Accuracy'].idxmax()]\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" MEILLEUR MODÈLE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Nom: {best_overall['Model']}\")\n",
    "print(f\"Accuracy: {best_overall['Accuracy']:.4f}\")\n",
    "print(f\"F1-Score: {best_overall['F1']:.4f}\")\n",
    "print(\"\\n Fichiers générés:\")\n",
    "print(\"   - results.csv\")\n",
    "print(\"   - comparison.png\")\n",
    "print(\"   - confusion_matrices.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
